# 神经网络模型在MNIST与CIFAR-10数据集上的实验报告

## 实验概述
本实验对比了多种神经网络架构在MNIST和CIFAR-10数据集上的表现，包括纯MLP、纯CNN以及组合模型，并研究了不同优化器、激活函数和正则化技术对模型性能的影响。

## MNIST数据集实验结果

### 基础模型对比

#### 纯MLP模型
- **Sigmoid激活函数**：15轮后准确率87%，loss为1.6
- **ReLU激活函数**：15轮后准确率86%，loss为0.35，收敛更快

#### 纯CNN模型
- **Sigmoid激活函数**：15轮后准确率96%，loss为0.1
- **ReLU激活函数**：15轮后准确率98.8%，loss为0.036，收敛更快，准确率更高

**结论**：ReLU激活函数在两种模型上均表现更优，收敛速度更快，准确率更高。

### 优化器实验
- 使用Adam优化器后，训练极不稳定，出现震荡
- 两个模型的准确率均较低，特别是CNN模型
- **分析**：Adam对变化小的参数给予更大学习率，导致梯度太大
- **解决方案**：降低学习率

### 批归一化(Batch Normalization)效果
- **MLP+BN**：准确率大幅提升至96%，loss为1.5
- **CNN+BN**：准确率达到98%，loss为0.07
- 两种模型均有显著提升

### 模型综合比较
- CNN参数更少，训练更快，收敛更快，准确率更高
- 架构细节和训练曲线图已在代码中实现并在Jupyter中可视化

### 创新组合模型
在题目要求之外，尝试了CNN+MLP组合模型：
- 使用Adam优化器和ReLU激活函数
- 降低学习率，使用0.25的dropout
- 仅训练5轮就在测试集上达到99%准确率

## CIFAR-10数据集实验结果

### 基础模型表现
CIFAR-10数据集难度更高：
- **纯MLP模型**：15轮训练后准确率41%
- **纯CNN模型**：15轮训练后准确率58%

### 改进实验

#### CNN+MLP组合
- 达到73%的准确率

#### 引入BN和ResNet
- 训练集与测试集准确率差异约10%
- 引入weight-decay和全连接层前0.5的dropout
- 仍未完全消除训练集与测试集的准确率差异

#### 最终结果
- 15轮训练后：
  - 训练集准确率：95.5%
  - 测试集准确率：84.6%

## 总结
1. ReLU激活函数相比Sigmoid在收敛速度和准确率上均有优势
2. Batch Normalization能显著提升模型性能
3. CNN架构在图像分类任务上优于MLP，参数更少，效率更高
4. 组合模型和适当的正则化技术能进一步提升性能
5. 不同数据集的难度差异显著，需要针对性地调整模型架构和训练策略

详细实现代码和训练曲线请参考附带的Jupyter notebook文件。