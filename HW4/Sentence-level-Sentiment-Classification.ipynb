{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a869e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "ROOT = r\"./data/stanfordSentimentTreebank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b756605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 读文件\n",
    "phrase2id = {}\n",
    "with open(os.path.join(ROOT, \"dictionary.txt\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        phrase, pid = line.rstrip(\"\\n\").split(\"|\")\n",
    "        phrase2id[phrase] = int(pid)\n",
    "\n",
    "def prob2label(p):\n",
    "    if 0.0 <= p <= 0.2: return 1\n",
    "    if 0.2 < p <= 0.4:  return 2\n",
    "    if 0.4 < p <= 0.6:  return 3\n",
    "    if 0.6 < p <= 0.8:  return 4\n",
    "    return 5\n",
    "\n",
    "pid2label = {}\n",
    "with open(os.path.join(ROOT, \"sentiment_labels.txt\"), encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"|\")\n",
    "    next(reader)\n",
    "    for pid_str, prob_str in reader:\n",
    "        pid2label[int(pid_str)] = prob2label(float(prob_str))\n",
    "\n",
    "idx2sent = {}\n",
    "with open(os.path.join(ROOT, \"datasetSentences.txt\"), encoding=\"utf-8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sid_str, sent = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        idx2sent[int(sid_str)] = sent\n",
    "\n",
    "sid2split = {}\n",
    "with open(os.path.join(ROOT, \"datasetSplit.txt\"), encoding=\"utf-8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sid_str, sp_str = line.rstrip(\"\\n\").split(\",\")\n",
    "        sid2split[int(sid_str)] = int(sp_str)  # 1=train,2=test,3=dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672339ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 分词\n",
    "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def tokenize(text): return TOKEN_RE.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744c7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 构造样本（句级，避免子树）\n",
    "train, val, test = [], [], []\n",
    "for sid, sent in idx2sent.items():\n",
    "    pid = phrase2id.get(sent)\n",
    "    if pid is None or pid not in pid2label:  # 过滤无标签项\n",
    "        continue\n",
    "    y = pid2label[pid]\n",
    "    toks = tokenize(sent)\n",
    "    sp = sid2split[sid]\n",
    "    (train if sp==1 else test if sp==2 else val).append((toks, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e94427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 词表\n",
    "counter = Counter()\n",
    "for toks, _ in train: counter.update(toks)\n",
    "itos = [\"<unk>\", \"<pad>\"] + [w for w,_ in counter.most_common()]\n",
    "stoi = {w:i for i,w in enumerate(itos)}\n",
    "pad_idx = stoi[\"<pad>\"]\n",
    "def numericalize(tokens): return torch.tensor([stoi.get(t, stoi[\"<unk>\"]) for t in tokens], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5519ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes: 8117 1044 2125\n"
     ]
    }
   ],
   "source": [
    "# 5) Dataset/DataLoader\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, samples): self.samples = samples\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        toks, y = self.samples[i]\n",
    "        return numericalize(toks), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    xs = pad_sequence(xs, batch_first=True, padding_value=pad_idx)\n",
    "    ys = torch.stack(ys)\n",
    "    return xs, ys\n",
    "\n",
    "train_loader = DataLoader(SSTDataset(train), batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(SSTDataset(val),   batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(SSTDataset(test),  batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"sizes:\", len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc01c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([14746, 300])\n",
      "batch: torch.Size([64, 46]) 1 5 logits: torch.Size([64, 5])\n"
     ]
    }
   ],
   "source": [
    "# 6) 预训练词向量\n",
    "from pathlib import Path\n",
    "\n",
    "VECTOR_PATH = Path(\"./data/vector.txt\")  # 如需改路径，请修改此变量\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_vectors_flex(path, itos):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"向量文件不存在: {path}\")\n",
    "\n",
    "    # 读取前两行进行格式判定\n",
    "    with path.open(encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        first = f.readline().strip()\n",
    "        second = f.readline().strip()\n",
    "    first_parts = first.split() if first else []\n",
    "    second_parts = second.split() if second else []\n",
    "\n",
    "    # 情况1：word2vec 文本（第一行 header，第二行以词开头）\n",
    "    if (len(first_parts) >= 2 and is_float(first_parts[0]) and is_float(first_parts[1])\n",
    "        and len(second_parts) >= 2 and not is_float(second_parts[0]) and is_float(second_parts[1])):\n",
    "        from gensim.models.keyedvectors import KeyedVectors\n",
    "        kv = KeyedVectors.load_word2vec_format(str(path), binary=False)\n",
    "        dim = kv.vector_size\n",
    "        emb = torch.empty(len(itos), dim).uniform_(-0.05, 0.05)\n",
    "        for i, tok in enumerate(itos):\n",
    "            if tok in kv:\n",
    "                emb[i] = torch.tensor(kv[tok])\n",
    "        return emb, dim\n",
    "\n",
    "    # 情况2：GloVe 文本（无 header，每行以词开头）\n",
    "    if (len(first_parts) >= 2 and not is_float(first_parts[0]) and is_float(first_parts[1])):\n",
    "        from gensim.models.keyedvectors import KeyedVectors\n",
    "        # GloVe 风格无 header\n",
    "        kv = KeyedVectors.load_word2vec_format(str(path), binary=False, no_header=True)\n",
    "        dim = kv.vector_size\n",
    "        emb = torch.empty(len(itos), dim).uniform_(-0.05, 0.05)\n",
    "        for i, tok in enumerate(itos):\n",
    "            if tok in kv:\n",
    "                emb[i] = torch.tensor(kv[tok])\n",
    "        return emb, dim\n",
    "\n",
    "    # 情况3：纯矩阵（每行只有浮点数，无词）\n",
    "    rows = []\n",
    "    dim = None\n",
    "    with path.open(encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            if not all(is_float(x) for x in parts):\n",
    "                raise ValueError(\"无法识别向量文件格式：既不是 word2vec（有 header），也不是 GloVe（词+向量），也不是纯矩阵。\")\n",
    "            if dim is None:\n",
    "                dim = len(parts)\n",
    "            elif len(parts) != dim:\n",
    "                raise ValueError(\"纯矩阵每行维度不一致。\")\n",
    "            rows.append([float(x) for x in parts])\n",
    "    mat = torch.tensor(rows, dtype=torch.float32)\n",
    "    if mat.shape[0] != len(itos):\n",
    "        raise ValueError(f\"纯矩阵行数({mat.shape[0]})与词表大小({len(itos)})不一致，无法按顺序对齐。\")\n",
    "    return mat, dim\n",
    "\n",
    "# 加载向量\n",
    "try:\n",
    "    emb_matrix, embedding_dim = load_vectors_flex(VECTOR_PATH, itos)\n",
    "    print(\"Embedding matrix shape:\", emb_matrix.shape)\n",
    "except Exception as e:\n",
    "    print(\"向量文件解析失败:\", e)\n",
    "    embedding_dim = 100\n",
    "    emb_matrix = torch.empty(len(itos), embedding_dim).uniform_(-0.05, 0.05)\n",
    "\n",
    "# 构建模型并拷贝\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_classes=5, pad_idx=pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.encoder = nn.GRU(emb_dim, 128, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.encoder(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "model = TinyModel(len(itos), emb_matrix.shape[1])\n",
    "with torch.no_grad():\n",
    "    model.embedding.weight.copy_(emb_matrix)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "logits = model(xb)\n",
    "print(\"batch:\", xb.shape, yb.min().item(), yb.max().item(), \"logits:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d9cbfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=1.5543 train_acc=0.3198 val_loss=1.4002 val_acc=0.3477 lr=1.00e-03\n",
      "Epoch 2: train_loss=1.2965 train_acc=0.4218 val_loss=1.2755 val_acc=0.4330 lr=1.00e-03\n",
      "Epoch 3: train_loss=1.1860 train_acc=0.4779 val_loss=1.2986 val_acc=0.4167 lr=1.00e-03\n",
      "Epoch 4: train_loss=1.0887 train_acc=0.5246 val_loss=1.3222 val_acc=0.4128 lr=1.00e-03\n",
      "Epoch 5: train_loss=0.9796 train_acc=0.5784 val_loss=1.3692 val_acc=0.3918 lr=1.00e-03\n",
      "Epoch 6: train_loss=0.8833 train_acc=0.6189 val_loss=1.4887 val_acc=0.3937 lr=5.00e-04\n",
      "Epoch 7: train_loss=0.6313 train_acc=0.7551 val_loss=1.8970 val_acc=0.3956 lr=5.00e-04\n",
      "Epoch 8: train_loss=0.4212 train_acc=0.8470 val_loss=2.2039 val_acc=0.4013 lr=5.00e-04\n",
      "Epoch 9: train_loss=0.3199 train_acc=0.8924 val_loss=2.5465 val_acc=0.3918 lr=5.00e-04\n",
      "Epoch 10: train_loss=0.2684 train_acc=0.9046 val_loss=2.8975 val_acc=0.3803 lr=2.50e-04\n",
      "Final val_loss=2.8975 val_acc=0.3803\n"
     ]
    }
   ],
   "source": [
    "# 7) 训练循环（LSTM)\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_size=256, num_layers=1, num_classes=5, pad_idx=pad_idx, dropout=0.5, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        enc_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.layer_norm = nn.LayerNorm(enc_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_dim, num_classes)\n",
    "        self.pad_idx = pad_idx\n",
    "    def forward(self, x):\n",
    "        x_emb = self.embedding(x)\n",
    "        out, _ = self.lstm(x_emb)  # (B, T, enc_dim)\n",
    "        # 取每个样本的最后一个非 pad 时间步\n",
    "        mask = (x != self.pad_idx)\n",
    "        lengths = mask.sum(dim=1).clamp_min(1)  # (B,)\n",
    "        last_idx = lengths - 1  # (B,)\n",
    "        B = out.size(0)\n",
    "        feat = out[torch.arange(B, device=out.device), last_idx, :]  # (B, enc_dim)\n",
    "        feat = self.layer_norm(feat)\n",
    "        feat = self.dropout(feat)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# 初始化模型并拷贝预训练向量\n",
    "model = LSTMClassifier(len(itos), emb_matrix.shape[1], bidirectional=True).to(device)\n",
    "with torch.no_grad():\n",
    "    model.embedding.weight.copy_(emb_matrix.to(device))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=3, factor=0.5, threshold=0.0001, min_lr=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device) - 1  # 标签从 [1..5] 映射到 [0..4]\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss_sum += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total, loss_sum, correct = 0, 0.0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = (yb - 1).to(device)  # [0..4]\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    train_loss = loss_sum / max(total, 1)\n",
    "    train_acc = correct / max(total, 1)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} lr={current_lr:.2e}\")\n",
    "\n",
    "# 验证集指标\n",
    "val_loss, val_acc = evaluate(val_loader)\n",
    "print(f\"Final val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ec07a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss=2.7641 acc=0.3845 (N=2125)\n",
      "Samples (pred/label/len):\n",
      "{'pred': 2, 'label': 3, 'len': 6}\n",
      "{'pred': 3, 'label': 4, 'len': 21}\n",
      "{'pred': 4, 'label': 5, 'len': 26}\n",
      "{'pred': 4, 'label': 3, 'len': 27}\n",
      "{'pred': 5, 'label': 5, 'len': 9}\n",
      "{'pred': 2, 'label': 4, 'len': 20}\n",
      "{'pred': 3, 'label': 4, 'len': 23}\n",
      "{'pred': 4, 'label': 4, 'len': 7}\n"
     ]
    }
   ],
   "source": [
    "# 8) 测试集评估\n",
    "# 说明：使用已训练好的当前模型，在 test_loader 上计算损失与准确率，并展示少量预测样例。\n",
    "\n",
    "model.eval()\n",
    "import math\n",
    "\n",
    "def evaluate_on_test(loader):\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    samples = []  # 采样若干条用于展示\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb0_4 = (yb - 1).to(device)  # [0..4]\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb0_4)\n",
    "            loss_sum += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb0_4).sum().item()\n",
    "            total += xb.size(0)\n",
    "\n",
    "            # 收集前若干样本展示（最多 8 条）\n",
    "            if len(samples) < 8:\n",
    "                # 回到 CPU，取若干条\n",
    "                for i in range(min(xb.size(0), 8 - len(samples))):\n",
    "                    samples.append({\n",
    "                        'pred': int(preds[i].cpu().item()) + 1,\n",
    "                        'label': int(yb0_4[i].cpu().item()) + 1,\n",
    "                        'len': int((xb[i] != pad_idx).sum().cpu().item())\n",
    "                    })\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1), samples\n",
    "\n",
    "test_loss, test_acc, sample_preds = evaluate_on_test(test_loader)\n",
    "print(f\"Test: loss={test_loss:.4f} acc={test_acc:.4f} (N={len(test_loader.dataset)})\")\n",
    "print(\"Samples (pred/label/len):\")\n",
    "for s in sample_preds:\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
